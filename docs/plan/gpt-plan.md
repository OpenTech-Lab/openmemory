# ğŸ§  OpenMemory â€” Local AI Memory System via MCP

A lightweight, local, shared memory system for multiple AI tools and agents.

**OpenMemory** provides persistent, searchable, and controllable memory for AIs by running a local MCP server backed by PostgreSQL + OpenSearch.

Instead of heavy RAG pipelines that waste tokens and compute, this system focuses on:

> âš¡ Fast keyword/importance-based retrieval
> ğŸ’¾ Structured + semantic memory
> ğŸ”Œ MCP-native integration
> ğŸ  Fully local & privacy-first
> ğŸ³ Docker-ready

---

## âœ¨ Why OpenMemory?

Most AI tools:

* forget everything
* or use full RAG (slow + expensive)
* or send entire chat history (token waste)

### Problems with traditional RAG

| Issue                       | Result        |
| --------------------------- | ------------- |
| Vector search every message | slow          |
| Embed everything            | expensive     |
| Send large contexts         | token waste   |
| Hard to control recall      | noisy answers |

---

## ğŸ’¡ Our Approach

We **do NOT use full RAG**.

Instead we use:

### ğŸ”¹ Hybrid Memory Retrieval

1. **Keyword / importance indexing**
2. **BM25 lexical search**
3. **Optional vector similarity**
4. **Extract only exact message blocks**
5. **Return minimal context**

Result:

> Small, precise, fast memory recall

---

# ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Tool (Claude/GPT/etc)  â”‚
â”‚        via MCP client      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ MCP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       OpenMemory MCP       â”‚
â”‚        (Rust server)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PostgreSQL   â”‚ OpenSearch  â”‚
â”‚ structured   â”‚ search idx  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        Next.js Dashboard
```

---

# ğŸ§© Features

### Core

* Persistent AI memory
* MCP server
* Local-first (no cloud required)
* Multi-AI shared memory
* Switchable recall (on/off)

### Memory Types

* Conversations
* Notes
* Facts
* Summaries
* Tool outputs

### Retrieval

* keyword index (BM25)
* importance score
* optional embeddings
* fast extraction
* minimal token usage

### Dev Experience

* Monorepo (Turborepo)
* Docker Compose
* One command startup

---

# ğŸ›  Tech Stack

## Backend

* Rust (Axum)
* PostgreSQL
* OpenSearch
* MCP protocol

## Frontend

* Next.js (App Router)
* TailwindCSS
* shadcn/ui

## Infra

* Docker Compose
* Turborepo
* pnpm

---

# ğŸ“‚ Project Structure (Monorepo)

```
openmemory/
â”‚
â”œâ”€ apps/
â”‚   â”œâ”€ web/           # Next.js dashboard
â”‚   â””â”€ server/        # Rust MCP server
â”‚
â”œâ”€ packages/
â”‚   â”œâ”€ sdk/           # TS SDK for clients
â”‚   â””â”€ shared-types/
â”‚
â”œâ”€ docker/
â”‚
â”œâ”€ turbo.json
â”œâ”€ docker-compose.yml
â””â”€ README.md
```

---

# ğŸš€ Quick Start

## 1. Clone

```bash
git clone https://github.com/yourname/openmemory
cd openmemory
```

---

## 2. Start with Docker

```bash
docker compose up
```

Starts:

* postgres
* opensearch
* rust server
* nextjs dashboard

---

## 3. Open dashboard

```
http://localhost:3000
```

---

## 4. Add MCP server to your AI tool

Example:

```json
{
  "mcpServers": {
    "memory": {
      "command": "openmemory-server",
      "args": ["--port", "8080"]
    }
  }
}
```

---

# ğŸ”Œ Usage Flow

### Typical interaction

```
User â†’ AI Tool â†’ MCP â†’ OpenMemory
                    â†“
               search memories
                    â†“
              return top context
                    â†“
              AI generates answer
                    â†“
             save conversation
```

---

## Memory Switch

Users can toggle:

```
Memory: ON  â†’ recall + store
Memory: OFF â†’ ignore memory
```

Useful for:

* private chats
* temporary sessions
* testing

---

# ğŸ§  Memory Algorithm Design

## Goals

* fast (<10ms search)
* low token usage
* minimal embeddings
* high precision

---

## Storage Model

### PostgreSQL

```
memories
- id
- user_id
- content
- summary
- importance_score
- tags
- created_at
```

### OpenSearch index

```
content (BM25)
keywords
summary
tags
importance_score
```

---

# ğŸ” Retrieval Strategy (NOT pure RAG)

## Step 1 â€” Extract keywords

Use:

* TF-IDF
* RAKE
* KeyBERT
* or simple noun phrase extraction

Example:

```
"how to deploy docker on ubuntu"
â†’ ["docker", "deploy", "ubuntu"]
```

---

## Step 2 â€” Hybrid search

```
score =
  BM25 * 0.6
+ importance * 0.2
+ recency * 0.1
+ optional vector * 0.1
```

---

## Step 3 â€” Select only top blocks

Instead of full documents:

```
top 3â€“5 message chunks only
```

---

## Step 4 â€” Inject minimal context

```
<Memory>
â€¢ previous docker fix
â€¢ ubuntu install steps
</Memory>
```

---

# âš¡ Why this beats RAG

| Method       | Cost   | Speed  | Tokens    |
| ------------ | ------ | ------ | --------- |
| Full RAG     | high   | slow   | high      |
| Chat history | medium | medium | very high |
| OpenMemory   | low    | fast   | low       |

---

# ğŸ§ª Development

## Install deps

```bash
pnpm install
```

---

## Run everything

```bash
pnpm dev
```

(Turborepo runs server + web)

---

## Individual

### Web

```bash
pnpm --filter web dev
```

### Server

```bash
cargo run
```

---

# ğŸ§  MCP API Example

### Save memory

```json
{
  "type": "memory.save",
  "content": "User prefers docker compose for deployments",
  "importance": 0.9
}
```

### Search memory

```json
{
  "type": "memory.search",
  "query": "docker deployment setup"
}
```

---

# ğŸ” Privacy

* fully local
* no cloud
* no telemetry
* your data stays yours

---

# ğŸ“ˆ Roadmap

### v1

* MCP server
* search
* dashboard
* docker

### v2

* semantic embeddings
* auto summarization
* memory clustering
* tagging

### v3

* multi-device sync
* plugin system
* agent tools

---

# ğŸ§© Future Research Ideas

You may explore:

* Hybrid search (BM25 + vector)
* Importance scoring (Ebbinghaus forgetting curve)
* Memory decay
* Hierarchical memory (short/long term)
* LLM-based summarization
* Knowledge graph linking

References:

* Retrieval Augmented Generation
* BM25 ranking
* KeyBERT
* Memory consolidation (cognitive science)

---

# ğŸ¤ Contributing

PRs welcome!

```bash
pnpm build
pnpm lint
pnpm test
```

---

# ğŸ“œ License

MIT

